{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09529482",
   "metadata": {},
   "source": [
    "\n",
    "# Naive Bayes Text Classifier (from scratch, **no Laplace smoothing**)\n",
    "\n",
    "Beginner-friendly notebook that implements a tiny **multinomial Naive Bayes** classifier for text.\n",
    "We keep it pure NumPy + Python so you can see how it works under the hood.\n",
    "\n",
    "What you'll do:\n",
    "- Create a tiny toy text dataset (two classes)\n",
    "- Tokenize and build a vocabulary\n",
    "- Compute class priors and word likelihoods **without** smoothing\n",
    "- Make predictions and measure accuracy\n",
    "- See what goes wrong when a word has zero counts (probability zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "949fe395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35\n",
      "Class priors: [0.33333333 0.66666667]\n",
      "Test docs:\n",
      " - 'A great game of cricket last night' | true=0 pred=1\n",
      " - 'Tennis players train hard for tournaments' | true=0 pred=1\n",
      "Accuracy: 0.0\n",
      "\n",
      "Tokens causing zero-likelihood by class (if any):\n",
      "'A great game of cricket last night' -> {0: ['of'], 1: []}\n",
      "'Tennis players train hard for tournaments' -> {0: [], 1: []}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import math\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    # lowercase + keep only letters/numbers + split\n",
    "    return re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "\n",
    "# Tiny toy dataset\n",
    "docs = [\n",
    "    \"The team won the football match\",\n",
    "    \"A great game of cricket last night\",\n",
    "    \"He scored three goals in the league\",\n",
    "    \"Tennis players train hard for tournaments\",\n",
    "    \"New smartphone release features faster chip\",\n",
    "    \"The laptop battery life is excellent\",\n",
    "    \"Software update improves security and speed\",\n",
    "    \"AI model achieves state of the art results\"\n",
    "]\n",
    "# Labels: 0 = Sports, 1 = Tech\n",
    "y = np.array([0,0,0,0, 1,1,1,1])\n",
    "\n",
    "# train/test split (simple, fixed)\n",
    "idx = np.arange(len(docs))\n",
    "rng.shuffle(idx)\n",
    "split = int(0.75 * len(docs))\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "X_train = [docs[i] for i in train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test  = [docs[i] for i in test_idx]\n",
    "y_test  = y[test_idx]\n",
    "\n",
    "# Build vocabulary from training data\n",
    "vocab = {}\n",
    "def add_to_vocab(token):\n",
    "    if token not in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "tokenized_train = []\n",
    "for txt in X_train:\n",
    "    toks = simple_tokenize(txt)\n",
    "    tokenized_train.append(toks)\n",
    "    for t in toks:\n",
    "        add_to_vocab(t)\n",
    "\n",
    "V = len(vocab)\n",
    "C = len(np.unique(y_train))\n",
    "\n",
    "# Count words per class (multinomial NB)\n",
    "word_counts = np.zeros((C, V), dtype=np.int64)    # counts of token j in class c\n",
    "class_counts = np.zeros(C, dtype=np.int64)        # total tokens in class c\n",
    "doc_counts = np.zeros(C, dtype=np.int64)          # number of docs in class c\n",
    "\n",
    "for toks, label in zip(tokenized_train, y_train):\n",
    "    doc_counts[label] += 1\n",
    "    for t in toks:\n",
    "        j = vocab[t]\n",
    "        word_counts[label, j] += 1\n",
    "        class_counts[label] += 1\n",
    "\n",
    "# Priors P(class)\n",
    "priors = doc_counts / doc_counts.sum()\n",
    "\n",
    "# Likelihoods P(word|class) WITHOUT smoothing â€” beware of zeros!\n",
    "# prob[c, j] = count(word j in class c) / total tokens in class c\n",
    "prob = np.zeros_like(word_counts, dtype=np.float64)\n",
    "for c in range(C):\n",
    "    denom = max(class_counts[c], 1)  # avoid divide-by-zero if empty class\n",
    "    prob[c, :] = word_counts[c, :] / denom\n",
    "\n",
    "def vectorize(text):\n",
    "    vec = np.zeros(V, dtype=np.int64)\n",
    "    for t in simple_tokenize(text):\n",
    "        if t in vocab:  # ignore OOV words (unseen in training)\n",
    "            vec[vocab[t]] += 1\n",
    "    return vec\n",
    "\n",
    "def predict(text):\n",
    "    x = vectorize(text)\n",
    "    # log P(class) + sum_j x_j * log P(word_j | class)\n",
    "    scores = []\n",
    "    for c in range(C):\n",
    "        # If any prob is zero where x_j > 0 => log(0) -> -inf\n",
    "        loglik = 0.0\n",
    "        for j, count in enumerate(x):\n",
    "            if count > 0:\n",
    "                p = prob[c, j]\n",
    "                if p <= 0.0:\n",
    "                    loglik = -np.inf\n",
    "                    break\n",
    "                loglik += count * math.log(p)\n",
    "        score = math.log(max(priors[c], 1e-12)) + loglik\n",
    "        scores.append(score)\n",
    "    return int(np.argmax(scores))\n",
    "\n",
    "# Evaluate\n",
    "y_pred = np.array([predict(t) for t in X_test])\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(\"Vocabulary size:\", V)\n",
    "print(\"Class priors:\", priors)\n",
    "print(\"Test docs:\")\n",
    "for t, yt, yp in zip(X_test, y_test, y_pred):\n",
    "    print(f\" - '{t}' | true={yt} pred={yp}\")\n",
    "print(\"Accuracy:\", round(float(acc), 3))\n",
    "\n",
    "# Show where zero-probability bites us (if any test word never seen in a class)\n",
    "zero_issues = []\n",
    "for t in X_test:\n",
    "    toks = simple_tokenize(t)\n",
    "    unseen_by_class = {0: [], 1: []}\n",
    "    for token in toks:\n",
    "        if token in vocab:\n",
    "            j = vocab[token]\n",
    "            for c in range(C):\n",
    "                if prob[c, j] == 0.0:\n",
    "                    unseen_by_class[c].append(token)\n",
    "    zero_issues.append((t, unseen_by_class))\n",
    "\n",
    "print(\"\\nTokens causing zero-likelihood by class (if any):\")\n",
    "for t, issues in zero_issues:\n",
    "    print(f\"'{t}' ->\", issues)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
