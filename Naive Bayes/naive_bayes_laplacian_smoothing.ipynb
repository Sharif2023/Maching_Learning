{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8beac3cf",
   "metadata": {},
   "source": [
    "\n",
    "# Naive Bayes with **Laplace (add-1) smoothing**\n",
    "\n",
    "This notebook is the same tiny multinomial Naive Bayes classifier as before,\n",
    "but now we apply **Laplace correction** to avoid zero probabilities.\n",
    "\n",
    "What changes with Laplace?\n",
    "- We add 1 to every word count before normalizing.\n",
    "- Denominator becomes (total tokens in class + |V|).\n",
    "- This prevents log(0) during prediction and usually improves robustness on small data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e3708cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 35\n",
      "Class priors: [0.33333333 0.66666667]\n",
      "Test docs:\n",
      " - 'A great game of cricket last night' | true=0 pred=1\n",
      " - 'Tennis players train hard for tournaments' | true=0 pred=1\n",
      "Accuracy: 0.0\n",
      "\n",
      "Tokens causing zero-likelihood by class (should be empty):\n",
      "'A great game of cricket last night' -> {0: [], 1: []}\n",
      "'Tennis players train hard for tournaments' -> {0: [], 1: []}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import math\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return re.findall(r\"[a-z0-9]+\", text.lower())\n",
    "\n",
    "# Same tiny dataset\n",
    "docs = [\n",
    "    \"The team won the football match\",\n",
    "    \"A great game of cricket last night\",\n",
    "    \"He scored three goals in the league\",\n",
    "    \"Tennis players train hard for tournaments\",\n",
    "    \"New smartphone release features faster chip\",\n",
    "    \"The laptop battery life is excellent\",\n",
    "    \"Software update improves security and speed\",\n",
    "    \"AI model achieves state of the art results\"\n",
    "]\n",
    "y = np.array([0,0,0,0, 1,1,1,1])  # 0=Sports, 1=Tech\n",
    "\n",
    "idx = np.arange(len(docs))\n",
    "rng.shuffle(idx)\n",
    "split = int(0.75 * len(docs))\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "X_train = [docs[i] for i in train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test  = [docs[i] for i in test_idx]\n",
    "y_test  = y[test_idx]\n",
    "\n",
    "# Build vocabulary on train\n",
    "vocab = {}\n",
    "def add_to_vocab(token):\n",
    "    if token not in vocab:\n",
    "        vocab[token] = len(vocab)\n",
    "\n",
    "tokenized_train = []\n",
    "for txt in X_train:\n",
    "    toks = simple_tokenize(txt)\n",
    "    tokenized_train.append(toks)\n",
    "    for t in toks:\n",
    "        add_to_vocab(t)\n",
    "\n",
    "V = len(vocab)\n",
    "C = len(np.unique(y_train))\n",
    "\n",
    "# Count words per class\n",
    "word_counts = np.zeros((C, V), dtype=np.int64)\n",
    "class_counts = np.zeros(C, dtype=np.int64)\n",
    "doc_counts = np.zeros(C, dtype=np.int64)\n",
    "\n",
    "for toks, label in zip(tokenized_train, y_train):\n",
    "    doc_counts[label] += 1\n",
    "    for t in toks:\n",
    "        j = vocab[t]\n",
    "        word_counts[label, j] += 1\n",
    "        class_counts[label] += 1\n",
    "\n",
    "# Priors\n",
    "priors = doc_counts / doc_counts.sum()\n",
    "\n",
    "# Laplace smoothing (add-1)\n",
    "# prob[c, j] = (count + 1) / (total_tokens_in_class + V)\n",
    "prob = np.zeros_like(word_counts, dtype=np.float64)\n",
    "for c in range(C):\n",
    "    denom = class_counts[c] + V  # add |V|\n",
    "    prob[c, :] = (word_counts[c, :] + 1.0) / max(denom, 1)\n",
    "\n",
    "def vectorize(text):\n",
    "    vec = np.zeros(V, dtype=np.int64)\n",
    "    for t in simple_tokenize(text):\n",
    "        if t in vocab:\n",
    "            vec[vocab[t]] += 1\n",
    "    return vec\n",
    "\n",
    "def predict(text):\n",
    "    x = vectorize(text)\n",
    "    scores = []\n",
    "    for c in range(C):\n",
    "        loglik = 0.0\n",
    "        for j, count in enumerate(x):\n",
    "            if count > 0:\n",
    "                p = prob[c, j]\n",
    "                loglik += count * math.log(max(p, 1e-12))\n",
    "        score = math.log(max(priors[c], 1e-12)) + loglik\n",
    "        scores.append(score)\n",
    "    return int(np.argmax(scores))\n",
    "\n",
    "# Evaluate\n",
    "y_pred = np.array([predict(t) for t in X_test])\n",
    "acc = (y_pred == y_test).mean()\n",
    "print(\"Vocabulary size:\", V)\n",
    "print(\"Class priors:\", priors)\n",
    "print(\"Test docs:\")\n",
    "for t, yt, yp in zip(X_test, y_test, y_pred):\n",
    "    print(f\" - '{t}' | true={yt} pred={yp}\")\n",
    "print(\"Accuracy:\", round(float(acc), 3))\n",
    "\n",
    "# Show that zero-probability is gone\n",
    "zero_issues = []\n",
    "for t in X_test:\n",
    "    toks = simple_tokenize(t)\n",
    "    unseen_by_class = {0: [], 1: []}\n",
    "    for token in toks:\n",
    "        if token in vocab:\n",
    "            j = vocab[token]\n",
    "            for c in range(C):\n",
    "                if prob[c, j] == 0.0:\n",
    "                    unseen_by_class[c].append(token)\n",
    "    zero_issues.append((t, unseen_by_class))\n",
    "\n",
    "print(\"\\nTokens causing zero-likelihood by class (should be empty):\")\n",
    "for t, issues in zero_issues:\n",
    "    print(f\"'{t}' ->\", issues)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
